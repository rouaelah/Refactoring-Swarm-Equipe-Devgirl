import time
from pathlib import Path
from typing import Dict, Any, Tuple

from src.utils.logger import log_experiment, ActionType
from src.tools.testing import run_pytest_on_file, run_tests_in_directory
from src.tools.analysis import run_pylint_analysis
from src.tools.file_ops import safe_read_file

class JudgeAgent:
    def __init__(self, llm_client=None):
        self.llm_client = llm_client
        # Chargement du prompt de testeur sans passer par la s√©curit√© sandbox
        try:
            # On utilise le open() standard de Python car on est dans src/
            with open("src/prompts/testeur.md", "r", encoding="utf-8") as f:
                self.system_prompt = f.read()
        except Exception as e:
            print(f"‚ö†Ô∏è Impossible de charger le prompt testeur: {e}")
            self.system_prompt = "Tu es un expert QA. Analyse les erreurs de test."

    def run_tests(self, target: str) -> Tuple[bool, Dict[str, Any]]:
        """
        Ex√©cute les tests unitaires et logue le r√©sultat pour le Data Officer.
        """
        print(f"üß™ Judge: Ex√©cution des tests sur {target}...")
        
        # 1. Lancement de Pytest via Toolsmith
        import os
        if os.path.isfile(target):
            test_results = run_pytest_on_file(target, timeout=30)
        else:
            test_results = run_tests_in_directory(target, timeout=30)
        
        success = test_results.get("success", False)

        # 2. LOGGING OBLIGATOIRE (Crit√®re de notation 30%)
        # On utilise ActionType.EVALUATION ou DEBUG selon le r√©sultat
        log_experiment(
            agent_name="Judge_Agent",
            model_used="pytest_engine", 
            action=ActionType.DEBUG if not success else ActionType.EVALUATION,
            status="SUCCESS" if success else "FAILURE",
            details={
                "input_prompt": f"Validation technique de: {target}",
                "output_response": test_results.get("summary", "Pas de r√©sum√©"),
                "total_tests": test_results.get("total_tests", 0)
            }
        )
        
        return success, test_results

    def validate_improvement(self, file_path: str, old_score: float) -> Tuple[bool, float]:
        """
        V√©rifie si le score Pylint s'est am√©lior√© apr√®s correction.
        """
        print(f"üìä Judge: V√©rification de l'am√©lioration du score...")
        analysis = run_pylint_analysis(Path(file_path))
        new_score = analysis.get("score", 0.0)
        
        improved = new_score > old_score
        
        if improved:
            print(f"‚úÖ Am√©lioration confirm√©e: {old_score} -> {new_score}")
        else:
            print(f"‚ö†Ô∏è Pas d'am√©lioration notable du score: {new_score}")
            
        return improved, new_score

    def generate_failure_report(self, test_results: Dict[str, Any]) -> str:
        """
        En cas d'√©chec, pr√©pare un rapport d√©taill√© pour le Fixer (Feedback Loop).
        """
        summary = test_results.get("summary", "Erreur inconnue")
        details = test_results.get("details", [])
        
        report = f"ECHEC DES TESTS DETECT√â:\n{summary}\n\nD√âTAILS DES ERREURS:\n"
        
        # Extraction des 3 premi√®res erreurs pour ne pas saturer le prompt
        failures = [d for d in details if isinstance(d, dict) and d.get("outcome") == "failed"]
        for f in failures[:3]:
            report += f"- Test: {f.get('name')}\n  Message: {f.get('message')}\n"
            
        return report